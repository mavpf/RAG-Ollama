# RAG Application Using a LLM Running on Local Computer with Ollama and Langchain

This RAG (Retrieval-Augmented Generation) app is designed to scrap information from an URL, add it to a Vector Database (Chroma), and feed this information to Ollama Llama2, so it can be consumed using Natural Language.

The app is based on this [article](https://medium.com/rahasak/build-rag-application-using-a-llm-running-on-local-computer-with-ollama-and-langchain-e6513853fda0) from Medium, but I made some changes.

### To be added soon

- Add search history database, for context;
- Add file scrap.

## 1. Components

### Ollama

Ollama is a platform that offers local AI models that users can run on their own devices. This allows users to harness the power of AI without relying on cloud-based services, which can enhance privacy and security. Ollama provides models that can be fine-tuned for specific tasks, such as text generation, image processing, or natural language understanding, and these models are typically optimized for performance on consumer hardware.

By running AI models locally, Ollama addresses concerns about data privacy, latency, and dependency on internet connectivity. It's particularly useful for developers and organizations that need to maintain control over their data and processes while leveraging AI technology

<https://ollama.com/>

### LangChain

LangChain is an open-source framework designed to help developers build applications that incorporate large language models (LLMs) more effectively. It simplifies the process of integrating LLMs into various applications by providing tools and abstractions that allow developers to manage and optimize the interaction between the model and other components of the application, such as data sources, APIs, and user interfaces.

LangChain's primary features include:

Chains: LangChain provides a way to create sequences of calls (chains) where each step can involve different operations or model invocations. This is useful for building complex applications where different tasks need to be performed in a specific order.

Agents: LangChain includes support for creating agents that can autonomously make decisions based on model outputs and perform actions, such as querying databases or interacting with external systems.

Memory: The framework offers memory management tools that allow applications to maintain context between interactions with the language model, making it easier to build conversational agents that can hold long-term context or remember previous interactions.

Data Augmentation: LangChain helps in augmenting the input to language models with additional context or structured data, improving the relevance and accuracy of model outputs.

Integration: It integrates well with various LLM providers and other machine learning frameworks, making it easier to switch between models or combine different technologies.

LangChain is particularly useful for developers who want to build sophisticated applications that leverage LLMs for tasks such as natural language processing, chatbots, automated agents, and more.

<https://python.langchain.com/v0.2/docs/introduction/>

### Vector Database

A vector database is a specialized type of database optimized for storing, indexing, and querying high-dimensional vectors, which are mathematical representations of data points in multi-dimensional space. These vectors are often used to represent complex data types, such as text, images, audio, and other forms of unstructured data, in a format that allows for efficient similarity searches.

Key Concepts:

1. Vectors: Vectors are numerical representations of data points, typically generated by machine learning models like embeddings from neural networks. For example, a sentence might be transformed into a 300-dimensional vector where each dimension represents a specific feature or attribute of that sentence.

2. Similarity Search: The primary use case for vector databases is similarity search, where the goal is to find vectors in the database that are closest to a given query vector. This is useful in applications like recommendation systems, image search, natural language processing, and more.

3. High-Dimensional Space: Vectors often exist in high-dimensional space, meaning they have many dimensions (features). A vector database is optimized to handle the challenges of indexing and querying in this high-dimensional space.

4. Indexing Techniques: Vector databases use specialized indexing techniques like Approximate Nearest Neighbor (ANN) algorithms to quickly find similar vectors. These techniques are necessary because exact search methods become computationally expensive as the dimensionality of the data increases.

5. Use Cases:

- Search Engines: Matching user queries with relevant documents or items.
- Recommendation Systems: Finding similar products, movies, or content based on user preferences.
- Image and Video Search: Retrieving similar images or videos based on visual features.
- Natural Language Processing: Identifying semantically similar text or documents.

## 2. Application Steps

### Scrape Web Data

Use LangChain RecursiveUrlLoader to scrape data from the web as documents. RecursiveUrlLoader scrapes the given url recursively in to given max_depth and read the data on the web. This data used to create vector embedding and answer questions of the user.

### Create Vector Embedding

Using open-source HuggingFaceEmbedding model all-MiniLM-L6-v2, the text is converted into multidimensional vectors, which are essentially high-dimensional numerical representations capturing semantic meanings and contextual nuances. Once embedded, these data can be grouped, sorted, searched, and more. We can calculate the distance between two sentences to determine their degree of relatedness. Importantly, these operations transcend traditional database searches that rely on keywords, capturing instead the semantic closeness between sentences.

### Store Vector Embedding in Chroma

The generated vector embeddings are then stored in the Chroma vector database. Chroma(commonly referred to as ChromaDB) is an open-source embedding database that makes it easy to build LLM apps by storing and retrieving embeddings and their metadata, as well as documents and queries. Chroma efficiently handles these embeddings, allowing for quick retrieval and comparison of text-based data.

### User Questions

The system provides an API through which users can submit their questions related to the content of the URL defined.

First, the History Aware Retriever setup a history prompt template, providing as placeholder the chat history and the latest question. This configuration enables the system to handle incoming queries by reformulating them appropriately, ensuring that the reformulated questions are understandable on their own, thereby enhancing the relevance and accuracy of the system’s responses.

After that, the QA Chain is build, by specifying how the system should respond to inputs based on retrieved context.

Finally, everything is put together, where the History Aware Retriever first processes the query to incorporate any relevant historical context, and then the processed query is handled by the QA Chain to produce the final answer.

### Post Prompt to LLM

After generating the prompt, it is posted to the LLM (in our case, the Llama2 7B) through Langchain libraries Ollama(Langchain officially supports the Ollama with in langchain_community.llms). The LLM then finds the answer to the question based on the provided context. The create_retrieval_chain handles this function of posting the query to the LLM (behind the scenes, it uses Ollama’s REST APIs to submit the question).

### LLM Generate Answer

The LLM, utilizing the advanced capabilities of Meta’s Llama-2, processes the question within the context of the provided content. It then generates a response and sends it back.

## 3. The configuration file

The file `config.py` have all information needed to run the app, including the TARGET_URL where the scrap will take place.

```bash
# define init index
INIT_INDEX = os.getenv('INIT_INDEX', 'false').lower() == 'true'

# vector index persist directory
INDEX_PERSIST_DIRECTORY = os.getenv('INDEX_PERSIST_DIRECTORY', "./data/chromadb")

# target url to scrape
TARGET_URL =  os.getenv('TARGET_URL', "https://python.langchain.com/v0.2/docs/concepts/")
DEPTH = 2

# http api port
HTTP_PORT = os.getenv('HTTP_PORT', 7654)

# context

CONDENSED_QUESTION = os.getenv('CONDENSED_QUESTION', "Given a chat history and the latest user question "
    "which might reference context in the chat history, "
    "formulate a standalone question which can be understood "
    "without the chat history. Do NOT answer the question, "
    "just reformulate it if needed and otherwise return it as is.")

# prompt

SYSTEM_PROMPT = os.getenv('SYSTEM_PROMPT', "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}")
```

## 4. Run Application

### Ollama in Docker

Ollama original docker image doesn't contain the GPU package installed, so we will create a local image with it.

**_It's highly recommended to use a GPU to process a LLM, since the usage of regular CPU can take a lot of time._**

To create a local image, named ollama, first of all have the docker client installed and running, then execute the command bellow inside the directory containing the Dockerfile:

```bash
>> docker build -t ollama .

#After, run the following command to start the container:

>> docker run -d --gpus=all -v ${PWD}/data:/root/.ollama -p 11434:11434 --name ollama ollama

#Once the container is up, connect to it:

>> docker exec -it ollama bash
```

Then, inside the container, start the ollama, using llama2:

```bash
$ ollama run llama2

#If you want, ask him some generic questions in the console:

>>> what is facebook?
Facebook is a social media platform that allows users to connect with friends....

#Leave the prompt:

>>> /bye

#And leave the container

$ exit
```

**WARNING: `data`directory is where Ollama will keep all `llama2` information. If the files from this directory are erased, `llama2` will need a new installation.**

Ollama also exposes REST API (`api/generate`) to the llm which runs on port `11434`, so we can ask question via the REST API (e.g using `curl`)
ask question and get answer as streams
`"stream": true` will streams the output of llm (e.g send word by word as stream). We will use it to communicate with Python.

```bash
>> curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "what is docker?",
  "stream": true
}'
```

Now that we have our Ollama container up and running, let's install Python requirements.

### Python

All Python packages needed for the application are listed in the `requirements.txt` file, that will be used to install them.


```bash
# create a virtual environment
>> cd ollama
>> python -m venv .venv

# enable virtual environment
>> activate

# install dependencies
>> pip install -r requirements.txt
```

### Run the application

With all requirements done, it's time to start the app:

```bash
#enable virtual environment in ollama source directory
>> cd ollama
>> activate

#set env variable INIT_INDEX to create the INDEX for the first time.
#every time that you need to recreate, set the variable again, but keep in mind that previous data will be removed
>> set INIT_INDEX=true

#run the app
>> python api.py
```

### Post a question

Once the RAG application is running, I can submit questions related to the website scraped.

```bash
curl -i -XPOST "http://localhost:7654/api/question" \
--header "Content-Type: application/json" \
--data '
{
  "question": "what is a retriever?",
}
'

#return

{
  "answer": "A retriever in the context of LangChain is a component that retrieves documents or data from an external source, such as a database or web API, based on a query or prompt. The retrieved documents are then passed to a model, such as a language model (LLM), for processing and generation of output.\n\nThe retriever acts as a bridge between the external data source and the LLM, allowing the LLM to leverage external knowledge and information to enhance its performance and generate more accurate and relevant output. The retriever can be thought of as a \"fetcher\" or \"retrieval agent,\" tasked with identifying and retrieving relevant documents or data based on the query or prompt provided by the user.\n\nSome common types of retrievers used in LangChain include:\n\n1. Web retriever: retrieves documents from the web using a URL or search query.\n2. Database retriever: retrieves data from a database using a SQL query or key-value pair.\n3. File retriever: retrieves files from a file system or storage location.\n4. API retriever: retrieves data from an application programming interface (API) using a set of endpoints or queries.\n\nThe retriever component is an important part of the LangChain architecture, as it allows the LLM to access and process a wide range of data sources, enhancing its ability to generate high-quality output."
}